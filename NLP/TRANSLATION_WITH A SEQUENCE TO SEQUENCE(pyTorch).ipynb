{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Language:\n",
    "    def __init__(self, name) :\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:'SOS',1:'EOS'} # we should define start and enc token \n",
    "        self.n_words = 2 # SOS and EOS was counted\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word \n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('/Users/arian/Downloads/ML/EX/NLP/translation/data/eng-fra.txt') as f:\n",
    "    lines = f.read().strip().split('\\n')\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "input_lang = Language('eng')\n",
    "output_lang = Language('fra')\n",
    "pairs = pairs[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[0].startswith(eng_prefixes) # reducing samples\n",
    "pairs = [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in pairs :\n",
    "    input_lang.addSentence(pair[0])\n",
    "    output_lang.addSentence(pair[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True) # the input and output tensors are (batch, seq, feature)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input)) \n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRnn(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size,target_tensor=None):\n",
    "        super(DecoderRnn, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        for i in range(MAX_LENGTH):\n",
    "    \n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                \n",
    "            # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "            # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach() \n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx,:len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "    train_ds = TensorDataset(torch.LongTensor(input_ids),torch.LongTensor(target_ids))\n",
    "    train_sampler = RandomSampler(train_ds)\n",
    "    train_dataloader = DataLoader(train_ds, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        \n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=1):\n",
    "    \n",
    "    \n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print(f'loss in  epoch number {epoch} is {loss}')\n",
    "        \n",
    "\n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in  epoch number 1 is 3.951293706893921\n",
      "loss in  epoch number 2 is 3.3642537593841553\n",
      "loss in  epoch number 3 is 2.8080376386642456\n",
      "loss in  epoch number 4 is 2.284605026245117\n",
      "loss in  epoch number 5 is 1.865330457687378\n",
      "loss in  epoch number 6 is 1.598844289779663\n",
      "loss in  epoch number 7 is 1.4668562412261963\n",
      "loss in  epoch number 8 is 1.3499886393547058\n",
      "loss in  epoch number 9 is 1.2962356209754944\n",
      "loss in  epoch number 10 is 1.2164058685302734\n",
      "loss in  epoch number 11 is 1.1195417046546936\n",
      "loss in  epoch number 12 is 1.1318131685256958\n",
      "loss in  epoch number 13 is 1.1008946299552917\n",
      "loss in  epoch number 14 is 1.0648645162582397\n",
      "loss in  epoch number 15 is 1.0125495791435242\n",
      "loss in  epoch number 16 is 0.9629949033260345\n",
      "loss in  epoch number 17 is 0.9315615594387054\n",
      "loss in  epoch number 18 is 0.9223119914531708\n",
      "loss in  epoch number 19 is 0.9292579591274261\n",
      "loss in  epoch number 20 is 0.8849940001964569\n",
      "loss in  epoch number 21 is 0.8456110060214996\n",
      "loss in  epoch number 22 is 0.819848507642746\n",
      "loss in  epoch number 23 is 0.8020445704460144\n",
      "loss in  epoch number 24 is 0.8003634810447693\n",
      "loss in  epoch number 25 is 0.7794117331504822\n",
      "loss in  epoch number 26 is 0.7694905698299408\n",
      "loss in  epoch number 27 is 0.7432848215103149\n",
      "loss in  epoch number 28 is 0.709650456905365\n",
      "loss in  epoch number 29 is 0.7066511809825897\n",
      "loss in  epoch number 30 is 0.694312572479248\n",
      "loss in  epoch number 31 is 0.6868290901184082\n",
      "loss in  epoch number 32 is 0.6891790926456451\n",
      "loss in  epoch number 33 is 0.6613571345806122\n",
      "loss in  epoch number 34 is 0.6448624730110168\n",
      "loss in  epoch number 35 is 0.6407240927219391\n",
      "loss in  epoch number 36 is 0.6444055140018463\n",
      "loss in  epoch number 37 is 0.6307726204395294\n",
      "loss in  epoch number 38 is 0.6159427762031555\n",
      "loss in  epoch number 39 is 0.6140379905700684\n",
      "loss in  epoch number 40 is 0.6099803149700165\n",
      "loss in  epoch number 41 is 0.5883484184741974\n",
      "loss in  epoch number 42 is 0.5962530970573425\n",
      "loss in  epoch number 43 is 0.5770342648029327\n",
      "loss in  epoch number 44 is 0.564099907875061\n",
      "loss in  epoch number 45 is 0.5638599395751953\n",
      "loss in  epoch number 46 is 0.5496874749660492\n",
      "loss in  epoch number 47 is 0.5372015237808228\n",
      "loss in  epoch number 48 is 0.5263340175151825\n",
      "loss in  epoch number 49 is 0.5248787105083466\n",
      "loss in  epoch number 50 is 0.5145741701126099\n",
      "loss in  epoch number 51 is 0.5252553522586823\n",
      "loss in  epoch number 52 is 0.509739875793457\n",
      "loss in  epoch number 53 is 0.49679605662822723\n",
      "loss in  epoch number 54 is 0.4949634075164795\n",
      "loss in  epoch number 55 is 0.49001985788345337\n",
      "loss in  epoch number 56 is 0.48666396737098694\n",
      "loss in  epoch number 57 is 0.47548340260982513\n",
      "loss in  epoch number 58 is 0.4737692177295685\n",
      "loss in  epoch number 59 is 0.4494711309671402\n",
      "loss in  epoch number 60 is 0.4674947261810303\n",
      "loss in  epoch number 61 is 0.4617112725973129\n",
      "loss in  epoch number 62 is 0.4502382278442383\n",
      "loss in  epoch number 63 is 0.43830394744873047\n",
      "loss in  epoch number 64 is 0.4457542896270752\n",
      "loss in  epoch number 65 is 0.4375336170196533\n",
      "loss in  epoch number 66 is 0.44016651809215546\n",
      "loss in  epoch number 67 is 0.42033877968788147\n",
      "loss in  epoch number 68 is 0.41334185004234314\n",
      "loss in  epoch number 69 is 0.4217371493577957\n",
      "loss in  epoch number 70 is 0.41781777143478394\n",
      "loss in  epoch number 71 is 0.40506933629512787\n",
      "loss in  epoch number 72 is 0.4104811102151871\n",
      "loss in  epoch number 73 is 0.40557335317134857\n",
      "loss in  epoch number 74 is 0.40051408112049103\n",
      "loss in  epoch number 75 is 0.3932999223470688\n",
      "loss in  epoch number 76 is 0.3912806659936905\n",
      "loss in  epoch number 77 is 0.3880648612976074\n",
      "loss in  epoch number 78 is 0.3834819793701172\n",
      "loss in  epoch number 79 is 0.3795745372772217\n",
      "loss in  epoch number 80 is 0.3768557608127594\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "decoder = DecoderRnn(hidden_size, output_lang.n_words)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 80, print_every=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
